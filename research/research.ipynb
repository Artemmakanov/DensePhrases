{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/roseltorg_devs/research/developers/Artem-folder/DensePhrases/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModel\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1773.81it/s]\n"
     ]
    }
   ],
   "source": [
    "class Dataset:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        self.ids = []\n",
    "        self.contexts = []\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "        self.spans_input_ids = []\n",
    "        model_checkpoint = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "        \n",
    "    def load_ds(self, size=100):\n",
    "\n",
    "        id = 0\n",
    "        contexts = raw_datasets['train']['context'] + raw_datasets['validation']['context']\n",
    "        questions = raw_datasets['train']['question'] + raw_datasets['validation']['question']\n",
    "        answers = raw_datasets['train']['answers'] + raw_datasets['validation']['answers']\n",
    "        if size:\n",
    "            treshold = size\n",
    "            self.size = size\n",
    "        else:\n",
    "            treshold = len(contexts)\n",
    "        for i in tqdm(range(treshold)):\n",
    "            try:\n",
    "                self.ids.append(id)\n",
    "                id += 1\n",
    "                self.contexts.append(contexts[i])\n",
    "                self.questions.append(questions[i])\n",
    "                self.answers.append(answers[i]['text'])\n",
    "                answer_tokenized = self.tokenizer(answers[i]['text'][0])['input_ids'][1:]\n",
    "                context_tokenized = self.tokenizer(contexts[i])['input_ids'][1:]\n",
    "                start = context_tokenized.index(answer_tokenized[0])\n",
    "                end = context_tokenized.index(answer_tokenized[-1])\n",
    "                self.spans_input_ids.append({\n",
    "                    'start':start,\n",
    "                    'end':end\n",
    "                })\n",
    "            except:\n",
    "                None\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, ds, hidden_dim=264):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        model_checkpoint = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.model_1 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.model_2 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.ds = ds            \n",
    "\n",
    "    def create_dump(self):\n",
    "        token_id2cnt = defaultdict(int)\n",
    "        token_id2j = {}\n",
    "        j = 0\n",
    "        \n",
    "        for context in tqdm(self.ds.contexts):\n",
    "            input_ids = self.tokenizer(context, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "            last_hidden_state = self.model(**input_ids).last_hidden_state[0].detach().numpy()\n",
    "            for token_num, token_id in enumerate(input_ids['input_ids'][0]):\n",
    "                if not token_id in self.tokenizer.added_tokens_decoder.keys(): \n",
    "                    last_hidden_state_token = last_hidden_state[token_num].reshape((1, self.hidden_dim))\n",
    "                    \n",
    "                    token_id = token_id.item()\n",
    "                    if token_id in token_id2j:\n",
    "                        H[token_id2j[token_id]] += last_hidden_state_token[0]\n",
    "                    else:\n",
    "                        if j == 0:\n",
    "                            H = last_hidden_state_token\n",
    "                        else:\n",
    "                            print(last_hidden_state_token.shape, H.shape)\n",
    "                            j += 1\n",
    "                            H = np.vstack((H, last_hidden_state_token))\n",
    "                        token_id2j[token_id] = j\n",
    "                            \n",
    "                    token_id2cnt[token_id] += 1\n",
    "                \n",
    "        for token_id, cnt in token_id2cnt.items():\n",
    "            H[token_id2j[token_id]] = H[token_id2j[token_id]] / cnt\n",
    "        self.index = faiss.IndexFlatIP(self.hidden_dim)\n",
    "        self.index.add(H)\n",
    "        self.H = H\n",
    "\n",
    "    def predict(self, id, k=100, verbose=False):\n",
    "        question = self.ds.questions[id]\n",
    "        answer = self.ds.answers[id]\n",
    "        context = self.ds.contexts[id]\n",
    "        if verbose:\n",
    "            print(f\"Q: {question}\")\n",
    "            print(f\"C: {context}\")\n",
    "\n",
    "        input_ids = self.tokenizer(question, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        last_hidden_state_1 = self.model_1(**input_ids).last_hidden_state.detach().numpy()[0][0].reshape((1, self.hidden_dim))\n",
    "        last_hidden_state_2 = self.model_2(**input_ids).last_hidden_state.detach().numpy()[0][0].reshape((1, self.hidden_dim))\n",
    "        S_1, I_1 = self.index.search(last_hidden_state_1, k)\n",
    "        S_2, I_2 = self.index.search(last_hidden_state_2, k)\n",
    "        \n",
    "        context_ids =  self.tokenizer(context)['input_ids']\n",
    "        \n",
    "        answer_candidate2cumscore = {}\n",
    "        for num_i_1, i_1 in enumerate(I_1[0]):\n",
    "            for num_i_2, i_2 in enumerate(I_2[0]):\n",
    "                if i_1 in context_ids and i_2 in context_ids:\n",
    "                    start_index = context_ids.index(i_1)\n",
    "                    end_index = context_ids.index(i_2)\n",
    "                    if start_index <= end_index:\n",
    "                        answer_candidate_ids = context_ids[start_index:end_index]\n",
    "                        answer_candidate = self.tokenizer.decode(answer_candidate_ids)\n",
    "                        answer_candidate2cumscore[answer_candidate] = S_1[0][num_i_1] + S_2[0][num_i_2]\n",
    "                        \n",
    "        if answer_candidate2cumscore:\n",
    "            answer, score = sorted(answer_candidate2cumscore.items(), key=lambda x: -x[1])[0]\n",
    "            answer = re.sub(r'#', '', answer)\n",
    "            return answer, score\n",
    "        else:\n",
    "            return '',  0.\n",
    "\n",
    "    def evaluate(self, k=100):\n",
    "        tp = 0\n",
    "        for id in tqdm(range(self.size)):\n",
    "            answer, score = self.predict(id, k)\n",
    "            if answer == self.ds.answers[id]:\n",
    "                tp += 1\n",
    "        return tp / self.size\n",
    "                \n",
    "dataset = Dataset()\n",
    "dataset.load_ds(size=1000)\n",
    "model = Model(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:16<00:00, 62.22it/s]\n"
     ]
    }
   ],
   "source": [
    "model.create_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "C: Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('', 0.0)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(0, k=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 400/400 [00:58<00:00,  6.88it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PhraseModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, ds):\n",
    "        super(PhraseModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        model_checkpoint = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.model_1 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.model_2 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.ds = ds\n",
    "        self.softmax = torch.nn.Softmax()\n",
    "        \n",
    "\n",
    "    def forward(self, id):\n",
    "        context_ids = self.tokenizer(self.ds.contexts[id], truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        question_ids = self.tokenizer(self.ds.questions[id], truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        last_hidden_state = self.model(**context_ids).last_hidden_state[0]\n",
    "        q_start = self.model_1(**question_ids).last_hidden_state[0][0]\n",
    "        q_end = self.model_2(**question_ids).last_hidden_state[0][0]\n",
    "       \n",
    "        z_start = torch.matmul(last_hidden_state, q_start.T).reshape(-1)\n",
    "        P_start = self.softmax(z_start)\n",
    "        loss_start = P_start[self.ds.spans_input_ids[id]['start']]\n",
    "        z_end = torch.matmul(last_hidden_state, q_end.T).reshape(-1)\n",
    "        P_end = self.softmax(z_end)\n",
    "        loss_end = P_end[self.ds.spans_input_ids[id]['start']]\n",
    "        return loss_start + loss_end\n",
    "    \n",
    "phrasemodel = PhraseModel(hidden_dim=264, ds=dataset)\n",
    "optimizer = torch.optim.Adam(phrasemodel.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/roseltorg_devs/research/developers/Artem-folder/DensePhrases/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrasemodel(140)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QueryModel(torch.nn.Module):\n",
    "    def __init__(self, hidden_dim, H, ds):\n",
    "        super(QueryModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        model_checkpoint = \"DeepPavlov/distilrubert-tiny-cased-conversational-v1\"\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.model_1 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.model_2 = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.H = torch.Tensor(H)\n",
    "        self.ds = ds\n",
    "\n",
    "    def forward(self, id):\n",
    "        question = self.ds.questions[id]\n",
    "        context_ids = self.tokenizer(self.ds.contexts[id])['input_ids']\n",
    "        answer_ids = self.tokenizer(self.ds.answers[id])['input_ids']\n",
    "        k = 100\n",
    "        input_ids = self.tokenizer(question, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        last_hidden_state_1 = self.model_1(**input_ids).last_hidden_state[0][0].reshape((1, self.hidden_dim))\n",
    "        last_hidden_state_2 = self.model_2(**input_ids).last_hidden_state[0][0].reshape((1, self.hidden_dim))\n",
    "        dot1 = torch.matmul(self.H, last_hidden_state_1.T)\n",
    "        dot2 = torch.matmul(self.H, last_hidden_state_2.T)\n",
    "        \n",
    "        I_1 = np.argsort(-dot1.detach().numpy())[:k]\n",
    "        I_2 = np.argsort(-dot2.detach().numpy())[:k]\n",
    "        \n",
    "        S_1 = dot1[I_1]\n",
    "        S_2 = dot2[I_2]\n",
    "        \n",
    "        answer2cumscore = {}\n",
    "        answer_correct2cumscore = {}\n",
    "        for num_i_1, i_1 in enumerate(I_1[0]):\n",
    "            for num_i_2, i_2 in enumerate(I_2[0]):\n",
    "                if i_1 in context_ids and i_2 in context_ids:\n",
    "                    start_index = context_ids.index(i_1)\n",
    "                    end_index = context_ids.index(i_2)\n",
    "                    if start_index <= end_index:\n",
    "                        answer_candidate_ids = context_ids[start_index:end_index]\n",
    "                        answer_candidate = self.tokenizer.decode(answer_candidate_ids)\n",
    "                        answer2cumscore[answer_candidate] = torch.Tensor(S_1[0][num_i_1] + S_2[0][num_i_2])\n",
    "                        if answer_candidate_ids == answer_ids:\n",
    "                            answer_correct2cumscore[answer_candidate] = torch.Tensor(S_1[0][num_i_1] + S_2[0][num_i_2])\n",
    "        \n",
    "        print(answer2cumscore)\n",
    "        print(answer_correct2cumscore)\n",
    "        if answer_correct2cumscore:\n",
    "            scores_numenator = torch.vstack(tuple(answer_correct2cumscore.values()))\n",
    "        else:\n",
    "            scores_numenator = torch.Tensor([0.], )\n",
    "        if answer2cumscore:\n",
    "            scores_denominator = torch.vstack(tuple(answer2cumscore.values()))\n",
    "        else:\n",
    "            scores_denominator = torch.Tensor([0.])\n",
    "        numenator = torch.sum(torch.exp(scores_numenator))\n",
    "        denominator = torch.sum(torch.exp(scores_denominator))\n",
    "        loss = - torch.log(numenator / denominator)\n",
    "        return loss\n",
    "    \n",
    "querymodel = QueryModel(hidden_dim=264, H=model.H, ds=dataset)\n",
    "optimizer = torch.optim.Adam(querymodel.parameters(), lr=0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/roseltorg_devs/research/developers/Artem-folder/DensePhrases/research/research.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B109.120.189.14/home/ubuntu/roseltorg_devs/research/developers/Artem-folder/DensePhrases/research/research.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B109.120.189.14/home/ubuntu/roseltorg_devs/research/developers/Artem-folder/DensePhrases/research/research.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B109.120.189.14/home/ubuntu/roseltorg_devs/research/developers/Artem-folder/DensePhrases/research/research.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/roseltorg_devs/research/developers/Artem-folder/DensePhrases/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    523\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    524\u001b[0m )\n",
      "File \u001b[0;32m~/roseltorg_devs/research/developers/Artem-folder/DensePhrases/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m     tensors,\n\u001b[1;32m    268\u001b[0m     grad_tensors_,\n\u001b[1;32m    269\u001b[0m     retain_graph,\n\u001b[1;32m    270\u001b[0m     create_graph,\n\u001b[1;32m    271\u001b[0m     inputs,\n\u001b[1;32m    272\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    273\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    274\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "optimizer.zero_grad()\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dense",
   "language": "python",
   "name": "dense"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
